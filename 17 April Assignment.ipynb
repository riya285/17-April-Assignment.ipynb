{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da89af9f-9d3f-43cb-a229-875001a3cc8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. What is Gradient Boosting Regression?\n",
    "Answer:\n",
    "Gradient Boosting Regression is a machine learning technique that belongs to the ensemble learning family. It builds a predictive model in the form of an ensemble of weak learners, typically decision trees, and combines them to create a stronger predictive model. The algorithm sequentially adds weak learners to correct the errors of the existing ensemble. Gradient Boosting Regression minimizes a loss function (typically the mean squared error for regression problems) by using gradient descent optimization.\n",
    "\n",
    "Q2. Implement a simple gradient boosting algorithm from scratch using Python and NumPy. Use a simple regression problem as an example and train the model on a small dataset. Evaluate the model's performance using metrics such as mean squared error and R-squared.\n",
    "Certainly! Implementing a gradient boosting algorithm from scratch involves several steps. Here is a simplified example in Python using NumPy for a regression problem:\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate a simple dataset for regression\n",
    "np.random.seed(42)\n",
    "X = np.sort(5 * np.random.rand(80, 1), axis=0)\n",
    "y = np.sin(X).ravel() + np.random.normal(0, 0.1, X.shape[0])\n",
    "\n",
    "# Define the gradient boosting regression class\n",
    "class GradientBoostingRegressor:\n",
    "    def __init__(self, n_estimators=100, learning_rate=0.1, max_depth=3):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_depth = max_depth\n",
    "        self.trees = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Initialize with the mean of the target variable\n",
    "        self.base_prediction = np.mean(y)\n",
    "        residuals = y - self.base_prediction\n",
    "\n",
    "        for _ in range(self.n_estimators):\n",
    "            tree = DecisionTreeRegressor(max_depth=self.max_depth)\n",
    "            tree.fit(X, residuals)\n",
    "            self.trees.append(tree)\n",
    "\n",
    "            # Update residuals with the negative gradient\n",
    "            residuals -= self.learning_rate * tree.predict(X)\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Make predictions by summing up predictions from all trees\n",
    "        predictions = np.zeros(len(X))\n",
    "        for tree in self.trees:\n",
    "            predictions += self.learning_rate * tree.predict(X)\n",
    "        return predictions + self.base_prediction\n",
    "\n",
    "# Define a simple DecisionTreeRegressor\n",
    "class DecisionTreeRegressor:\n",
    "    def __init__(self, max_depth):\n",
    "        self.max_depth = max_depth\n",
    "        self.tree = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.tree = self.build_tree(X, y, depth=0)\n",
    "\n",
    "    def build_tree(self, X, y, depth):\n",
    "        # Implement tree building logic (e.g., recursive split based on mean)\n",
    "        # ...\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Implement tree traversal for predictions\n",
    "        # ...\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "split_idx = int(0.8 * len(X))\n",
    "X_train, X_test = X[:split_idx], X[split_idx:]\n",
    "y_train, y_test = y[:split_idx], y[split_idx:]\n",
    "\n",
    "# Train the gradient boosting model\n",
    "gb_model = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3)\n",
    "gb_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = gb_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "print(f\"R-squared: {r2}\")\n",
    "\n",
    "# Plot the results\n",
    "plt.scatter(X_test, y_test, color='black', label='True data')\n",
    "plt.scatter(X_test, y_pred, color='red', label='Predictions')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "Q3. Experiment with different hyperparameters such as learning rate, number of trees, and tree depth to optimise the performance of the model. Use grid search or random search to find the best hyperparameters.\n",
    "Certainly! For hyperparameter tuning, you can use grid search or random search to find the optimal combination. Here's an example using scikit-learn's GridSearchCV:\n",
    "\n",
    "\n",
    "    \n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [3, 5, 7]\n",
    "}\n",
    "\n",
    "# Create the GradientBoostingRegressor\n",
    "gb_model = GradientBoostingRegressor()\n",
    "\n",
    "# Create GridSearchCV\n",
    "grid_search = GridSearchCV(gb_model, param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = grid_search.best_params_\n",
    "print(f\"Best Hyperparameters: {best_params}\")\n",
    "\n",
    "# Evaluate the model with the best hyperparameters\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred_best = best_model.predict(X_test)\n",
    "\n",
    "# Calculate metrics\n",
    "mse_best = mean_squared_error(y_test, y_pred_best)\n",
    "r2_best = r2_score(y_test, y_pred_best)\n",
    "\n",
    "print(f\"Best Mean Squared Error: {mse_best}\")\n",
    "print(f\"Best R-squared: {r2_best}\")\n",
    "\n",
    "\n",
    "Q4. What is a weak learner in Gradient Boosting?\n",
    "Answer:\n",
    "In Gradient Boosting, a weak learner is a model that performs slightly better than random chance. It is typically a simple model with low complexity, such as a shallow decision tree. Weak learners are sequentially added to the ensemble to correct errors made by the previous learners.\n",
    "\n",
    "Q5. What is the intuition behind the Gradient Boosting algorithm?\n",
    "Answer:\n",
    "The intuition behind Gradient Boosting is to combine the predictions of weak learners in a sequential manner, with each new learner focusing on the mistakes of the ensemble so far. The algorithm minimizes a loss function by using gradient descent, updating the model with the negative gradient of the loss function with respect to the predictions.\n",
    "\n",
    "Q6. How does Gradient Boosting algorithm build an ensemble of weak learners?\n",
    "Answer:\n",
    "Gradient Boosting builds an ensemble by sequentially adding weak learners. At each iteration, a new weak learner is trained to predict the negative gradient of the loss function with respect to the current\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
